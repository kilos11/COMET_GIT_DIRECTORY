{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNo6WPUr/EVrltWlS3mxth1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"aynjEfRqId84","executionInfo":{"status":"ok","timestamp":1720441575571,"user_tz":-120,"elapsed":12529,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"}},"outputId":"95f06301-e0ba-422e-b253-b813570c1a8a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting comet_ml\n","  Downloading comet_ml-3.43.2-py3-none-any.whl (677 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/677.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m675.8/677.4 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.4/677.4 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting everett[ini]<3.2.0,>=1.0.1 (from comet_ml)\n","  Downloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\n","Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (4.19.2)\n","Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (5.9.5)\n","Collecting python-box<7.0.0 (from comet_ml)\n","  Downloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting requests-toolbelt>=0.8.0 (from comet_ml)\n","  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (2.31.0)\n","Collecting semantic-version>=2.8.0 (from comet_ml)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Collecting sentry-sdk>=1.1.0 (from comet_ml)\n","  Downloading sentry_sdk-2.8.0-py2.py3-none-any.whl (300 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.6/300.6 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting simplejson (from comet_ml)\n","  Downloading simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (2.0.7)\n","Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.14.1)\n","Collecting wurlitzer>=1.0.2 (from comet_ml)\n","  Downloading wurlitzer-3.1.1-py3-none-any.whl (8.6 kB)\n","Collecting dulwich!=0.20.33,>=0.20.6 (from comet_ml)\n","  Downloading dulwich-0.22.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (979 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m979.1/979.1 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (13.7.1)\n","Collecting configobj (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n","  Downloading configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.18.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (3.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (2024.6.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml) (0.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from configobj->everett[ini]<3.2.0,>=1.0.1->comet_ml) (1.16.0)\n","Installing collected packages: everett, wurlitzer, simplejson, sentry-sdk, semantic-version, python-box, dulwich, configobj, requests-toolbelt, comet_ml\n","  Attempting uninstall: python-box\n","    Found existing installation: python-box 7.2.0\n","    Uninstalling python-box-7.2.0:\n","      Successfully uninstalled python-box-7.2.0\n","Successfully installed comet_ml-3.43.2 configobj-5.0.8 dulwich-0.22.1 everett-3.1.0 python-box-6.1.0 requests-toolbelt-1.0.0 semantic-version-2.10.0 sentry-sdk-2.8.0 simplejson-3.19.2 wurlitzer-3.1.1\n"]}],"source":["!pip install comet_ml"]},{"cell_type":"markdown","source":["##Comet Optimizer is the component of the Comet platform that provides you with automated hyperparameter optimization for machine learning models with your chosen hyperparameter ranges and search algorithm.\n","\n","##`Grid search`, `random search`, and `Bayes optimization` are all supported by Comet Optimizer. You can learn more about each approach in our Hyperparameter Optimization With Comet blog post[link text](https://www.comet.com/site/blog/hyperparameter-optimization-with-comet/).\n","\n","##Comet Optimizer streamlines the process of finding the optimal set of hyperparameters for your ML experiments by offering a unified, framework-agnostic interface for all your workflows that seamlessly integrates with the experiment management capabilities offered by Comet Experiment Management[link text](https://www.comet.com/docs/v2/guides/experiment-management/quickstart/)."],"metadata":{"id":"0HaD-HZOIqK_"}},{"cell_type":"markdown","source":["##**Get started: Hyperparameter Tuning**##\n","##Hyperparameter tuning with Comet Optimizer is a simple two-step process which aims to configure and run an Optimizer object instance.\n","\n","##Comet Optimizer is accessible both from `Python SDK and CLI`. The step-by-step instructions below use the Python SDK, but you can refer to the end-to-end examples to explore both options."],"metadata":{"id":"5t9KqypvJttX"}},{"cell_type":"markdown","source":["###**Prerequisites: Choose your tuning strategy**###\n","##Before you begin optimization, you need to choose:\n","\n","###*The hyperparameters to tune.*\n","###*The search space for each hyperparameter to tune.*\n","###*The search algorithm `(one of grid search, random search, and Bayesian optimization)`.*\n","###*Additionally, make sure to refactor your existing model training code so that hyperparameters are defined as parametrized variables.*"],"metadata":{"id":"AwEZRdWvKPcW"}},{"cell_type":"markdown","source":["###**Define the Optimizer configuration**###\n","##`Optimizer` accesses your hyperparameter search options from a config dictionary that you define.\n","\n","##Comet uses the config dictionary to dynamically find the best set of hyperparameter values that will minimize or maximize a particular metric of your choice.\n","\n","##You can define the dictionary config in code or in a `.JSON` config file. For example, you could define your configuration dictionary in code as:"],"metadata":{"id":"rMzBbNVIK9Mo"}},{"cell_type":"markdown","source":["##On top of these three mandatory configuration keys, the Optimizer configuration dictionary also supports the optional keys `name` and `trials`. You can find a full description of each configuration key in the Configure the Optimizer page[link text](https://www.comet.com/docs/v2/guides/optimizer/configure-optimizer/)."],"metadata":{"id":"_4WqnPvfSxkz"}},{"cell_type":"markdown","source":["##This configuration dictionary is setting up a hyperparameter optimization process using Bayesian optimization. Here's a breakdown of what each part does:\n","\n","##1. The \"algorithm\" key specifies that Bayesian optimization will be used for tuning the hyperparameters.\n","\n","##2. The \"parameters\" section defines the hyperparameters to be optimized:\n","   **- \"learning_rate\" is set as an integer type with log-uniform scaling, ranging from 0.00001 to 0.001.**\n","      **- \"batch_size\" is set as a discrete parameter with specific values to choose from: 32, 64, 128, and 256.**\n","\n","      **3. The \"spec\" section defines the optimization objective:**\n","         - The \"metric\" to be optimized is accuracy.\n","            - The \"objective\" is to maximize this accuracy.\n","\n","            ##This configuration would typically be used with a hyperparameter optimization library or framework to automatically search for the best combination of learning rate and batch size that maximizes the model's accuracy. The Bayesian optimization algorithm will intelligently explore the defined parameter space to find the optimal configuration efficiently."],"metadata":{"id":"giuq0ZACVrtj"}},{"source":["config = {\n","    # Pick the algorithm:\n","    # Specifies the Bayesian optimization algorithm for\n","    # hyperparameter tuning\n","    \"algorithm\": \"bayes\",\n","\n","    # Declare your hyperparameters:\n","    \"parameters\": {\n","        # Defines the learning rate parameter with\n","        #log-uniform scaling between 0.00001 and 0.001\n","        \"learning_rate\": {\"type\": \"integer\",\n","                          \"scaling_type\": \"log_uniform\",\n","                          \"min\": 0.00001, \"max\": 0.001},\n","\n","        # Defines the batch size parameter with discrete\n","        #values of 32, 64, 128, and 256\n","        \"batch_size\": {\"type\": \"discrete\",\n","                       \"values\": [32, 64, 128, 256]},  # Moved batch_size inside parameters\n","    },\n","    # Declare what to optimize, and how:\n","    \"spec\": {\n","        # Specifies that accuracy is the metric to be optimized\n","        \"metric\": \"accuracy\",\n","        # Indicates that the goal is to maximize the accuracy\n","        \"objective\": \"maximize\",\n","    },\n","}\n","\n","import comet_ml\n","\n","comet_ml.Optimizer(config=config)"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5GWH88-jUJf1","executionInfo":{"status":"ok","timestamp":1720441681543,"user_tz":-120,"elapsed":631,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"}},"outputId":"d53ab6dd-7344-4b1e-fee4-05cf73c86b13"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[1;38;5;39mCOMET INFO:\u001b[0m 128413b6629540b7849228029a0a5550\n","\u001b[1;38;5;39mCOMET INFO:\u001b[0m Using optimizer config: {'algorithm': 'bayes', 'configSpaceSize': 'infinite', 'endTime': None, 'id': '128413b6629540b7849228029a0a5550', 'lastUpdateTime': None, 'maxCombo': 0, 'name': '128413b6629540b7849228029a0a5550', 'parameters': {'batch_size': {'type': 'discrete', 'values': [32, 64, 128, 256]}, 'learning_rate': {'max': 0.001, 'min': 1e-05, 'scalingType': 'uniform', 'scaling_type': 'log_uniform', 'type': 'integer'}}, 'predictor': None, 'spec': {'gridSize': 10, 'maxCombo': 0, 'metric': 'accuracy', 'minSampleSize': 100, 'objective': 'maximize', 'retryAssignLimit': 0, 'retryLimit': 1000}, 'startTime': 17622562192, 'state': {'mode': None, 'seed': None, 'sequence': [], 'sequence_i': 0, 'sequence_pid': None, 'sequence_retry': 0, 'sequence_retry_count': 0}, 'status': 'running', 'suggestion_count': 0, 'trials': 1, 'version': '2.0.26'}\n"]},{"output_type":"execute_result","data":{"text/plain":["<comet_ml.optimizer.Optimizer at 0x7d474562b850>"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["###**Run optimization**###\n","##The `Optimizer` configuration defined in the previous step is used to initialize the Optimizer object[link text](https://www.comet.com/docs/v2/api-and-sdk/python-sdk/reference/Optimizer/#optimizerinit), e.g. by running:"],"metadata":{"id":"6kh6rPFrTRgi"}},{"cell_type":"code","source":["import comet_ml\n","\n","opt = comet_ml.Optimizer(config=config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yLgkE_DuTznn","executionInfo":{"status":"ok","timestamp":1720443016832,"user_tz":-120,"elapsed":998,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"}},"outputId":"5e358451-6ce4-45ee-fe4f-bed01701d432"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[1;38;5;39mCOMET INFO:\u001b[0m 9ad3fe6af6eb4a42840d77e382e37c7b\n","\u001b[1;38;5;39mCOMET INFO:\u001b[0m Using optimizer config: {'algorithm': 'bayes', 'configSpaceSize': 'infinite', 'endTime': None, 'id': '9ad3fe6af6eb4a42840d77e382e37c7b', 'lastUpdateTime': None, 'maxCombo': 0, 'name': '9ad3fe6af6eb4a42840d77e382e37c7b', 'parameters': {'batch_size': {'type': 'discrete', 'values': [32, 64, 128, 256]}, 'learning_rate': {'max': 0.001, 'min': 1e-05, 'scalingType': 'uniform', 'scaling_type': 'log_uniform', 'type': 'integer'}}, 'predictor': None, 'spec': {'gridSize': 10, 'maxCombo': 0, 'metric': 'accuracy', 'minSampleSize': 100, 'objective': 'maximize', 'retryAssignLimit': 0, 'retryLimit': 1000}, 'startTime': 3280722827, 'state': {'mode': None, 'seed': None, 'sequence': [], 'sequence_i': 0, 'sequence_pid': None, 'sequence_retry': 0, 'sequence_retry_count': 0}, 'status': 'running', 'suggestion_count': 0, 'trials': 1, 'version': '2.0.26'}\n"]}]},{"cell_type":"markdown","source":["##Under the hood, the `Optimizer` object creates one `Experiment` object per tuning run, i.e. for each automated hyperparameter selection.\n","\n","##You can then perform your tuning trials by iterating through the tuning runs with `opt.get_experiments()`. Within the scope of each tuning experiment, you can train and evaluate your models as you normally do but providing the parameters selected by Optimizer with the experiment.`get_parameter()` method. For example, the pseudocode below showcases how to run tuning experiments against the `learning_rate` and `batch_size` parameters:"],"metadata":{"id":"m0D5rW_aUtGY"}},{"cell_type":"code","source":["# Iterate through all experiments generated by the optimizer\n","for exp in opt.get_experiments():\n","    # Create a new model instance with hyperparameters from the current experiment\n","    model = my_model(\n","        # Get the learning rate for this experiment\n","        learning_rate=exp.get_parameter(\"learning_rate\"),\n","        # Get the batch size for this experiment\n","        batch_size=exp.get_parameter(\"batch_size\"),\n","    )\n","\n","    # Train and evaluate the model here\n","    # Placeholder for actual training and evaluation code\n","    loss, _ = train_and_evaluate(model, ...)\n","\n","    # Log the loss metric for this experiment\n","    exp.log_metric(\"loss\", loss)\n","\n","    # Save and log the trained model, using a unique identifier based on the optimizer's ID\n","    exp.log_model(f\"model_{opt.get_id()}\", model)\n","\n","    # End the current experiment to finalize logging and free up resources\n","    exp.end()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C1ZTvEIJWUjh","executionInfo":{"status":"ok","timestamp":1720443024700,"user_tz":-120,"elapsed":518,"user":{"displayName":"KGAOGELO Moloko","userId":"02299502165535871727"}},"outputId":"edb8a601-7bfb-4222-ff15-617f94d2a153"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[1;38;5;39mCOMET INFO:\u001b[0m Optimizer search 9ad3fe6af6eb4a42840d77e382e37c7b has completed\n"]}]},{"cell_type":"markdown","source":["##Comet Optimizer logs your optimizer configuration in the `Other tab` of the Single Experiment page with the prefix `\"optimizer_\"`.\n","\n","##We recommend also saving the optimizer parameters with `Experiment.log_parameters() (or Experiment.log_parameter())` and optimizer metrics with Experiment.`log_metrics() (or Experiment.log_metric())` so that they are also accessible in the Hyperparameters tab[link text](https://www.comet.com/docs/v2/guides/comet-ui/experiment-management/single-experiment-page/#hyperparameters-tab) and Metrics[link text](https://www.comet.com/docs/v2/guides/comet-ui/experiment-management/single-experiment-page/#metrics-tab) tab respectively of the Single Experiment page.\n","\n","##You can then navigate the results of a Comet Optimizer run as you would with any Comet ML Project containing multiple experiments. Learn more in the Analyze hyperparameter tuning results page[link text](https://www.comet.com/docs/v2/guides/optimizer/analyze-results/)."],"metadata":{"id":"Q_oOUdZsaN74"}}]}